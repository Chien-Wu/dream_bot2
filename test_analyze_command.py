#!/usr/bin/env python3
"""
Test script for the /analyze command AI functionality.
"""

import sys
sys.path.append('.')

from unittest.mock import Mock, patch
from src.services.admin_command_service import AdminCommandService
from src.models import AIResponse


def test_analyze_command():
    """Test the analyze command functionality."""
    print("ğŸ§ª Testing /analyze Command AI Integration")
    print("=" * 50)
    
    # Create mock services
    mock_db = Mock()
    mock_line = Mock()
    
    # Mock failed questions data
    mock_failed_questions = [
        {
            'content': 'å¦‚ä½•ç”³è«‹ç¤¾æœƒç¦åˆ©è£œåŠ©ï¼Ÿ',
            'confidence': 0.45,
            'frequency': 3,
            'ai_response': 'éœ€è¦æº–å‚™ç›¸é—œæ–‡ä»¶...',
            'created_at': '2024-01-15'
        },
        {
            'content': 'èº«å¿ƒéšœç¤™è­‰æ˜è¦æ€éº¼è¾¦ç†ï¼Ÿ',
            'confidence': 0.52,
            'frequency': 2,
            'ai_response': 'è«‹è¯çµ¡ç›¸é—œå–®ä½...',
            'created_at': '2024-01-14'
        }
    ]
    
    # Mock database query to return failed questions
    mock_db.execute_query.return_value = [
        mock_failed_questions[0],
        mock_failed_questions[1]
    ]
    
    # Create admin service
    admin_service = AdminCommandService(mock_db, mock_line)
    
    # Test 1: Check if completion API method exists
    print("\n--- Test 1: Completion API Method ---")
    try:
        if hasattr(admin_service, '_call_openai_completion'):
            print("âœ… Completion API method exists")
        else:
            print("âŒ Completion API method missing")
    except Exception as e:
        print(f"âŒ Method check error: {e}")
    
    # Test 2: Test with mock completion API
    print("\n--- Test 2: Mock Completion API ---")
    try:
        # Mock the completion API call
        expected_result = """**ä¸»è¦å•é¡Œé¡å‹ï¼š** ç¤¾æœƒç¦åˆ©ç”³è«‹ã€èº«å¿ƒéšœç¤™æœå‹™ã€æ”¿åºœè£œåŠ©

**æœ€é‡è¦å•é¡Œç¯„ä¾‹ï¼š**
1. å¦‚ä½•ç”³è«‹ç¤¾æœƒç¦åˆ©è£œåŠ©ï¼Ÿ
2. èº«å¿ƒéšœç¤™è­‰æ˜è¦æ€éº¼è¾¦ç†ï¼Ÿ
3. é•·æœŸç…§è­·æœå‹™ç”³è«‹æµç¨‹
4. å…’ç«¥ç™¼å±•é²ç·©æ—©æœŸç™‚è‚²
5. å¼±å‹¢å®¶åº­æ€¥é›£æ•‘åŠ©"""
        
        # Patch the completion API method
        with patch.object(admin_service, '_call_openai_completion', return_value=expected_result):
            # Test the analysis method directly
            result = admin_service._analyze_questions_with_ai(mock_failed_questions, 7)
            
            print(f"âœ… AI Analysis Result:")
            print(f"   Length: {len(result)} characters")
            print(f"   Contains 'ä¸»è¦å•é¡Œé¡å‹': {'ä¸»è¦å•é¡Œé¡å‹' in result}")
            print(f"   Contains 'æœ€é‡è¦å•é¡Œç¯„ä¾‹': {'æœ€é‡è¦å•é¡Œç¯„ä¾‹' in result}")
            print(f"   First 100 chars: {result[:100]}...")
        
    except Exception as e:
        print(f"âŒ Mock completion API error: {e}")
        import traceback
        traceback.print_exc()
    
    # Test 3: Test full command execution
    print("\n--- Test 3: Full Command Execution ---")
    try:
        # Execute the analyze command
        result = admin_service.execute_command("analyze", ["7", "50"])
        
        print(f"Command execution result:")
        print(f"   Success: {result.success}")
        print(f"   Message length: {len(result.message)} characters")
        print(f"   Has data: {result.data is not None}")
        print(f"   First 200 chars: {result.message[:200]}...")
        
        if not result.success:
            print(f"   Error: {result.error}")
        
    except Exception as e:
        print(f"âŒ Full command execution error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… Testing completed!")


def test_simplified_analyze():
    """Test a simplified version to isolate issues."""
    print("\nğŸ”§ Testing Simplified Analysis Logic")
    print("=" * 50)
    
    # Test the core logic without dependencies
    questions = [
        {'content': 'æ¸¬è©¦å•é¡Œ1', 'confidence': 0.4, 'frequency': 2},
        {'content': 'æ¸¬è©¦å•é¡Œ2', 'confidence': 0.5, 'frequency': 1}
    ]
    
    # Test prompt generation
    questions_text = ""
    for i, q in enumerate(questions, 1):
        content = q.get('content', '')[:200]
        confidence = q.get('confidence', 0)
        frequency = q.get('frequency', 1)
        questions_text += f"{i}. [{confidence:.2f}] [å‡ºç¾{frequency}æ¬¡] {content}\n"
    
    analysis_prompt = f"""åˆ†æä»¥ä¸‹ {len(questions)} å€‹ä½ä¿¡å¿ƒåº¦å•é¡Œï¼ˆéå»7å¤©å…§ï¼‰ï¼Œè«‹æä¾›ç°¡æ½”åˆ†æï¼š

å•é¡Œåˆ—è¡¨ï¼š
{questions_text}

è«‹æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

**ä¸»è¦å•é¡Œé¡å‹ï¼š** [é¡å‹1]ã€[é¡å‹2]ã€[é¡å‹3]

**æœ€é‡è¦å•é¡Œç¯„ä¾‹ï¼š**
1. [æœ€é‡è¦çš„å•é¡Œ1]
2. [æœ€é‡è¦çš„å•é¡Œ2] 
3. [æœ€é‡è¦çš„å•é¡Œ3]
4. [æœ€é‡è¦çš„å•é¡Œ4]
5. [æœ€é‡è¦çš„å•é¡Œ5]

è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¿æŒç°¡æ½”ã€‚"""
    
    print(f"âœ… Generated prompt:")
    print(f"   Length: {len(analysis_prompt)} characters")
    print(f"   Contains questions: {len(questions)} questions included")
    print(f"   Sample: {analysis_prompt[:300]}...")


if __name__ == "__main__":
    try:
        test_analyze_command()
        test_simplified_analyze()
    except KeyboardInterrupt:
        print("\nâš ï¸  Testing interrupted")
    except Exception as e:
        print(f"\nğŸ’¥ Testing failed: {e}")
        import traceback
        traceback.print_exc()